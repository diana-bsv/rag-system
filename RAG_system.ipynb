{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diana-bsv/rag-system/blob/main/RAG_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, DPRContextEncoder, pipeline\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "sfgj8OlEt-hA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1acg_SJkJ-NC",
        "outputId": "563427d9-bf76-4503-de96-fa18d52e9213"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model for encoding the context\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\", device = device)\n",
        "model = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\")\n"
      ],
      "metadata": {
        "id": "7yym9g2J81lD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model for encoding the question\n",
        "q_tokenizer = AutoTokenizer.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\", device = device)\n",
        "q_model = AutoModel.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")"
      ],
      "metadata": {
        "id": "zLgfWpllalDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model for context question answering\n",
        "pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", device = device)"
      ],
      "metadata": {
        "id": "yP1hfEmn3ek-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_chunks(path):\n",
        "  f = open(path, \"r\")\n",
        "  data = f.read()\n",
        "  f.close()\n",
        "\n",
        "  words = data.split()\n",
        "\n",
        "  chunks = []\n",
        "\n",
        "  for i in range(0, len(words), 100):\n",
        "    tmp = \" \".join(words[i:i+100])\n",
        "\n",
        "    tmp.strip()\n",
        "\n",
        "    if tmp:\n",
        "      chunks.append(tmp)\n",
        "\n",
        "  print(f\"Number of chunks: {len(chunks)}\")\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "42EUaiaR7gHu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_chunks(chunks, tokenizer, model):\n",
        "\n",
        "  tokenized = tokenizer(chunks, add_special_tokens=True, return_token_type_ids=False, padding=True)\n",
        "\n",
        "\n",
        "  input_ids = torch.tensor(tokenized.input_ids)\n",
        "  attention_mask = torch.tensor(tokenized.attention_mask)\n",
        "\n",
        "  model = model.to(device)\n",
        "\n",
        "  last_hidden_states = []\n",
        "  with torch.no_grad():\n",
        "    for i in tqdm(range(0, len(input_ids), 100)):\n",
        "      tmp1 = input_ids[i:i+100].to(device)\n",
        "      tmp2 = attention_mask[i:i+100].to(device)\n",
        "      last_hidden_states.append(model(tmp1, attention_mask=tmp2).pooler_output.detach().cpu())\n",
        "\n",
        "\n",
        "  features = torch.cat(last_hidden_states, dim=0)\n",
        "  print(f\"Done\\nShape of features array: {features.shape}\")\n",
        "\n",
        "  return features\n"
      ],
      "metadata": {
        "id": "5O4N1H_17A4Y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_question(question, q_tokenizer, q_model):\n",
        "  q_tokenized = q_tokenizer(question, add_special_tokens=True, return_token_type_ids=False)\n",
        "  id = torch.tensor([q_tokenized.input_ids])\n",
        "  mask = torch.tensor([q_tokenized.attention_mask])\n",
        "\n",
        "  outp = q_model(id, attention_mask=mask).pooler_output.detach()\n",
        "\n",
        "  return outp[0]\n"
      ],
      "metadata": {
        "id": "EVkTcAVC94AK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_nearest_chunks(features, question, n_neighbors=5):\n",
        "\n",
        "  features = features.numpy()\n",
        "\n",
        "  nn_model = NearestNeighbors(n_neighbors=n_neighbors, metric=\"cosine\")\n",
        "  nn_model.fit(features)\n",
        "\n",
        "  target_vector = question.reshape(1, -1)\n",
        "\n",
        "  distances, indices = nn_model.kneighbors(target_vector)\n",
        "\n",
        "  # print(f\"Closest chunks indices {indices}\")\n",
        "  # print()\n",
        "  # print(f\"Distanses {distances}\")\n",
        "\n",
        "  return indices[0].tolist()"
      ],
      "metadata": {
        "id": "744h3D9h2jlD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer(question, chunks, features, pipe, NN, q_tokenizer, q_model):\n",
        "\n",
        "  question_vector = vectorize_question(question, q_tokenizer, q_model)\n",
        "  indices = find_nearest_chunks(features, question_vector, n_neighbors=NN)\n",
        "\n",
        "  relevant = [chunks[i] for i in indices]\n",
        "\n",
        "  prompt = \"Source: \" + \" \".join(relevant[:5]) + \"\\n\\nUsing the source answer the question: \" + question\n",
        "\n",
        "\n",
        "  answer = pipe(prompt)[0][\"generated_text\"]\n",
        "\n",
        "  print(f\"Q: {question}\")\n",
        "  print(f\"A: {answer}\")\n",
        "  print()\n",
        "  print(\"Based on:\")\n",
        "\n",
        "  for line in relevant:\n",
        "    print(line)\n"
      ],
      "metadata": {
        "id": "EPxAyRGMFqrF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fwL4JGo0Jhe8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hjTLffaXJhh1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"./Looking-for-Alaska.txt\"\n",
        "NN = 3 # Number of nearest neighbours\n",
        "\n",
        "chunks = make_chunks(PATH)\n",
        "\n",
        "features = vectorize_chunks(chunks, tokenizer, model)"
      ],
      "metadata": {
        "id": "-BgHdAPy8B3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#True\n",
        "question = \"How to get out of the labyrinth?\"\n",
        "\n",
        "answer(question, chunks, features, pipe, NN, q_tokenizer, q_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWg8qh5YIi2B",
        "outputId": "c6554923-515c-46b6-9d4d-e9cfd634f3a8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: How to get out of the labyrinth?\n",
            "A: straight and fast\n",
            "\n",
            "Based on:\n",
            "at that moment reaching the finish line. The rest was darkness. \"Damn it,\" he sighed. \"How will I ever get out of this labyrinth!\" The whole passage was underlined in bleeding, water-soaked black ink. But there was another ink, this one a crisp blue, post-flood, and an arrow led from \"How will I ever get out of this labyrinth!\" to a margin note written in her loop-heavy cursive: Straight &amp; Fast. \"Hey, she wrote something in here after the flood,\" I said. \"But it's weird. Look. Page one ninety-two.\" I tossed the book to the Colonel, and he flipped to\n",
            "put my soul to rest.\" \"Or hers,\" I said. \"Right. I'd forgotten about her.\" He shook his head. \"That keeps happening.\" \"Well, you have to write something,\"I argued. \"After all this time, it still seems to me like straight and fast is the only way out — but I choose the labyrinth. The labyrinth blows, but I choose it.\" one hundred thirty-six days after Two weeks later,I still hadn't finished my final for the Old Man, and the semester was just twenty-four hours from ending. I was walking home from my final test, a difficult but ultimately (I hoped) successful\n",
            "short. I must talk, and you must listen, for we are engaged here in the most important pursuit in history: the search for meaning. What is the nature of being a person? What is the best way to go about being a person? How did we come to be, and what will become of us when we are no longer? In short: What are the rules of this game, and how might we best play it?\" The nature of the labyrinth,I scribbled into my spiral notebook, and the way out of it.This teacher rocked. I hated discussion classes. I hated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Wrong\n",
        "question = \"Who is Alaska?\"\n",
        "\n",
        "answer(question, chunks, features, pipe, NN, q_tokenizer, q_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayaS529XIkxi",
        "outputId": "70dfb747-a052-4b55-de4b-446e182da97e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Who is Alaska?\n",
            "A: Flow Reeda\n",
            "\n",
            "Based on:\n",
            "were me, you, and Jake,\" I said to him. \"And we don't know. So how the hell are we going to find out?\" Takumi looked over at the Colonel and sighed. \"I don't think it would help, to know where she was going. I think that would make it worse for us. Just a gut feeling.\" \"Well, mygut wants to know,\" Lara said, and only then did I realize what Takumi meant the day we'd showered together — I may have kissed her, but I really didn'thave a monopoly on Alaska; the Colonel and I weren't the only ones who\n",
            "kids she did not consider Weekday Warriors and piled us into her tiny blue two-door. By happy coincidence, a cute sophomore named Lara ended up sitting on my lap. Lara'd been born in Russia or someplace, and she spoke with a slight accent. Since we were only four layers of clothes from doing it, I took the opportunity to introduce myself. \"I know who you are.\" She smiled. \"You're Alaska's freend from Flow Reeda.\" \"Yup. Get ready for a lot of dumb questions, 'cause I suck at precalc,\" I said. She started to answer, but then she was thrown back\n",
            "since there's a new moon. We're staying at the barn. You, me, the Colonel, Takumi, and, as a special gift to you, Pudge, Lara Buterskaya.\" \"The Lara Buterskaya I puked on?\" \"She's just shy. She still likes you.\" Alaska laughed. \"Puking made you look — vulnerable.\" \"Very perky boobs,\" the Colonel said. \"Are you bringing Takumi for me?\" \"You need to be single for a while.\" \"True enough,\" the Colonel said. \"Just spend a few more months playing video games,\" she said. \"That hand-eye coordination will come in handy when you get to third base.\" \"Gosh, I haven't heard the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#True\n",
        "question = \"Where is Colonel from?\"\n",
        "\n",
        "answer(question, chunks, features, pipe, NN, q_tokenizer, q_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCOSYL9BIm2C",
        "outputId": "470723e2-d413-48ac-9e05-1dea5d17a17d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Where is Colonel from?\n",
            "A: New Hope, Alabama\n",
            "\n",
            "Based on:\n",
            "\"Uh, no. Whatever is fine.\" \"I see you've decorated the place,\" he said, gesturing toward the world map. \"I like it.\" And then he started naming countries. He spoke in a monotone, as if he'd done it a thousand times before. Afghanistan. Albania. Algeria. American Samoa. Andorra. And so on. He got through the A'sbefore looking up and noticing my incredulous stare. \"I could do the rest, but it'd probably bore you. Something I learned over the summer. God, you can't imagine how boring New Hope, Alabama, is in the summertime. Like watching soybeans grow. Where are you from, by\n",
            "Colonel behind the wheel of Takumi's SUV. We asked Lara and Takumi to come along, but they were tired of chasing ghosts, and besides, finals were coming. It was a bright afternoon, and the sun bore down on the asphalt so that the ribbon of road before us quivered with heat. We drove a mile down Highway 119 and then merged onto I-65 northbound, heading toward the accident scene and Vine Station. The Colonel drove fast, and we were quiet, staring straight ahead. I tried to imagine what she might have been thinking, trying again to see through time and\n",
            "\"Well,now it's war,\"the Colonel shouted the next morning. I rolled over and looked at the clock: 7:52. My first Culver Creek class, French, started in eighteen minutes. I blinked a couple times and looked up at the Colonel, who was standing between the couch and the coffee table, holding his well-worn, once-white tennis shoes by the laces. For a long time, he stared at me, and I stared at him. And then, almost in slow motion, a grin crept across the Colonel's face. \"I've got to hand it to them,\" he said finally. \"That was pretty clever.\" \"What?\" I asked.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#True\n",
        "question = \"Who are Colonel's parents?\"\n",
        "\n",
        "answer(question, chunks, features, pipe, NN, q_tokenizer, q_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEmePBo3Iog6",
        "outputId": "e5145109-ebd8-4dcd-d0ef-21716adf76b6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Who are Colonel's parents?\n",
            "A: Dolores\n",
            "\n",
            "Based on:\n",
            "embarrassed of his mom at all. He was just scared that we would act like condescending boarding-school snobs. I'd always found the Colonel's I-hate-the-rich routine a little overwrought until I saw him with his mom. He was the same Colonel, but in a totally different context. It made me hope that one day, I could meet Alaska's family, too. Dolores insisted that Alaska and I share the bed, and she slept on the pull-out while the Colonel was out in his tent. I worried he would get cold, but frankly I wasn't about to give up my bed with Alaska.\n",
            "Colonel was short — he couldn't afford to be any taller. The place was really one long room, with a full-size bed in the front, a kitchenette, and a living area in the back with a TV and a small bathroom — so small that in order to take a shower, you pretty much had to sit on the toilet. \"It ain't much,\" the Colonel's mom (\"That's Dolores, not Miss Martin\") told us. \"But y'alls a-gonna have a turkey the size o' the kitchen.\" She laughed. The Colonel ushered us out of the trailer immediately after our brief tour, and\n",
            "Colonel never introduced me to her and didn't have a chance to that night. \"Oh. My God. Can't you at least press your shirt?\" she asked, even though the Colonel was standing in front of the ironing board. \"We're going out with my parents.\"Sara looked awfully nice in her blue summer dress. Her long, pale blond hair was pulled up into a twist, with a strand of hair falling down each side of her face. She looked like a movie star — a bitchy one. \"Look, I did my best. We don't all have maids to do our ironing.\" \"Chip,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#True\n",
        "question = \"What is the сapital of Uzbekistan?\"\n",
        "\n",
        "answer(question, chunks, features, pipe, NN, q_tokenizer, q_model)"
      ],
      "metadata": {
        "id": "OVHmybJ0Io7I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4af62e91-c560-4e9a-e97e-90a481af13f8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What is the сapital of Uzbekistan?\n",
            "A: Tashkent\n",
            "\n",
            "Based on:\n",
            "and Alaska started calling them bufriedos, and then everyone did, and then finally Maureen officially changed the name.\" He paused. \"I don't know what to do, Miles.\" \"Yeah. I know.\" \"I finished memorizing the capitals,\" he said. \"Of the states?\" \"No. That was fifth grade. Of the countries. Name a country.\" \"Canada,\" I said. \"Something hard.\" \"Urn. Uzbekistan?\" \"Tashkent.\" He didn't even take a moment to think. It was just there, at the tip of his tongue, as if he'd been waiting for me to say \"Uzbekistan\" all along. \"Let's smoke.\" We walked to the bathroom and turned on the\n",
            "so little, and I grabbed it tight, his cold seeping into me and my warmth into him. \"I memorized the populations,\" he said. \"Uzbekistan.\" \"Twenty-four million seven hundred fifty-five thousand five hundred and nineteen.\" \"Cameroon,\" I said, but it was too late. He was asleep, his hand limp in mine. I placed it back under the quilt and climbed up into his bed, a top-bunk man for this night at least. I fell asleep listening to his slow, even breaths, his stubbornness finally melting away in the face of insurmountable fatigue. six days after That Sunday,Igot up after three hours\n",
            "grief is not something nameless Buddhists, Christians, and Muslims have to explore. The questions of religious thought have become, I suspect, personal.\" He shuffled through our exams, pulling one out from the pile before him. \"I have here Alaska's final. You'll recall that you were asked what the most important question facing people is, and how the three traditions we're studying this year address that question. This was Alaska's question.\" With a sigh, he grabbed hold of his chair and lifted himself out of it, then wrote on the blackboard: How will we ever get out of this labyrinth of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Wrong\n",
        "question = \"What is the сapital of Great Britain?\"\n",
        "\n",
        "answer(question, chunks, features, pipe, NN, q_tokenizer, q_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no7jjSTpIqpF",
        "outputId": "a6a6954a-b5c4-4c86-fcf3-51032f5fc378"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What is the сapital of Great Britain?\n",
            "A: a general named Sedgwick said, 'They couldn't hit an elephant from\n",
            "\n",
            "Based on:\n",
            "and Alaska started calling them bufriedos, and then everyone did, and then finally Maureen officially changed the name.\" He paused. \"I don't know what to do, Miles.\" \"Yeah. I know.\" \"I finished memorizing the capitals,\" he said. \"Of the states?\" \"No. That was fifth grade. Of the countries. Name a country.\" \"Canada,\" I said. \"Something hard.\" \"Urn. Uzbekistan?\" \"Tashkent.\" He didn't even take a moment to think. It was just there, at the tip of his tongue, as if he'd been waiting for me to say \"Uzbekistan\" all along. \"Let's smoke.\" We walked to the bathroom and turned on the\n",
            "ultimate nobility: King of Kings. Class over. You can pick up a copy of your final exam on the way out. Stay dry.\" It wasn't until I stood up to leave that I noticed Alaska had skipped class — how could she skip the only class worth attending? I grabbed a copy of the final for her. The final exam: What is the most important question human beings must answer? Choose your question wisely, and then examine how Islam, Buddhism, and Christianity attempt to answer it\"I hope that poor bastard lives the rest of the school year,\" the Colonel said\n",
            "revolutionaries, it would seem, died with flair. I read the last words out loud to Lara. She turned on her side, placing her head on my chest. \"Why do you like last words so much?\" Strange as it might seem, I'd never really thought about why. \"I don't know,\" I said, placing my hand against the small of her back. \"Sometimes, just because they're funny. Like in the Civil War, a general named Sedgwick said, 'They couldn't hit an elephant from this dis—' and then he got shot.\" She laughed. \"But a lot of times, people die how they live.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qb8s0bSjT8um"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}